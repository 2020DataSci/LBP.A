{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Process\n",
    "\n",
    "For the embedding process we have chosen a method known as the \"Continuous Bag of Words\" (CBOW). The CBOW model attempts to predict a target word based on context words (surrounding words). For example if we have a scentence, \"Beauty is in the eye of the beholder\", we can form pairs of surrounding words and our target word. The size of this pair is dependent on the size of our context window (a numeric value n that considers n amount of context words around our target word). If we chose a context window size equal to two then an example pair would be ([Beauty,in],is). The model would then try to predict the tarket word. This model structure is referenced from the website: https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html.\n",
    "\n",
    "The first step in this process is having an accurate unique word count of our refined corpus which we will do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>listens</th>\n",
       "      <th>hotness</th>\n",
       "      <th>genres</th>\n",
       "      <th>genius ID</th>\n",
       "      <th>spotify ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fast Cars</td>\n",
       "      <td>Craig David</td>\n",
       "      <td>Fast cars Fast women Speed bikes with the n...</td>\n",
       "      <td>751624</td>\n",
       "      <td>28</td>\n",
       "      <td>['R&amp;B Genius', 'Rock Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Watching The Rain</td>\n",
       "      <td>Scapegoat Wax</td>\n",
       "      <td>Hello hello its me again  You know since yo...</td>\n",
       "      <td>10681</td>\n",
       "      <td>6</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Run to the Money</td>\n",
       "      <td>Bankroll Fresh</td>\n",
       "      <td>Fuckin up money I already done it Guess ...</td>\n",
       "      <td>160202</td>\n",
       "      <td>17</td>\n",
       "      <td>['Rap Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Determined (Vows of Vengeance) - Live at Summe...</td>\n",
       "      <td>Kataklysm</td>\n",
       "      <td>Your fate is made of words that you spill Li...</td>\n",
       "      <td>19218</td>\n",
       "      <td>4</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Dandelion Days</td>\n",
       "      <td>Adam McHeffey</td>\n",
       "      <td>Well look back on our old ways And call them...</td>\n",
       "      <td>1038</td>\n",
       "      <td>0</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36754</td>\n",
       "      <td>36754</td>\n",
       "      <td>36754</td>\n",
       "      <td>Rain</td>\n",
       "      <td>Leela James</td>\n",
       "      <td>I cant take this rain  no When Im bringing ...</td>\n",
       "      <td>256830</td>\n",
       "      <td>19</td>\n",
       "      <td>['Soul', 'R&amp;B Genius']</td>\n",
       "      <td>957879.0</td>\n",
       "      <td>58qUsOq28QtDpPdyHAARNQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36755</td>\n",
       "      <td>36755</td>\n",
       "      <td>36755</td>\n",
       "      <td>Cardiff Afterlife</td>\n",
       "      <td>Manic Street Preachers</td>\n",
       "      <td>If the love between us has faded away Left ...</td>\n",
       "      <td>94223</td>\n",
       "      <td>15</td>\n",
       "      <td>['Rock Genius']</td>\n",
       "      <td>463900.0</td>\n",
       "      <td>1SylCJI2pHEXC1GO3cd9iR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36756</td>\n",
       "      <td>36756</td>\n",
       "      <td>36756</td>\n",
       "      <td>Darkness Breeds Immortality</td>\n",
       "      <td>Marduk</td>\n",
       "      <td>An angel in ecstasy crosses the sky The sky ...</td>\n",
       "      <td>69664</td>\n",
       "      <td>17</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>1402388.0</td>\n",
       "      <td>5RFS3Z2vLi0jOzSDAIHQnd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36757</td>\n",
       "      <td>36757</td>\n",
       "      <td>36757</td>\n",
       "      <td>Super Bad Sisters</td>\n",
       "      <td>Sister Sledge</td>\n",
       "      <td>Super bad super bad ah ah ah arent we bad S...</td>\n",
       "      <td>78248</td>\n",
       "      <td>14</td>\n",
       "      <td>['R&amp;B Genius']</td>\n",
       "      <td>3988510.0</td>\n",
       "      <td>3n2aVCZTCCJ4HDIrsiTbZ4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36758</td>\n",
       "      <td>36758</td>\n",
       "      <td>36758</td>\n",
       "      <td>Topsy's Revenge</td>\n",
       "      <td>Grand Archives</td>\n",
       "      <td>The rumours around I wont last long now My c...</td>\n",
       "      <td>57110</td>\n",
       "      <td>4</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>1098025.0</td>\n",
       "      <td>70ua2E92cg7euTplfFAvYm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36759 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  \\\n",
       "0               0             0   \n",
       "1               1             1   \n",
       "2               2             2   \n",
       "3               3             3   \n",
       "4               4             4   \n",
       "...           ...           ...   \n",
       "36754       36754         36754   \n",
       "36755       36755         36755   \n",
       "36756       36756         36756   \n",
       "36757       36757         36757   \n",
       "36758       36758         36758   \n",
       "\n",
       "                                                   title  \\\n",
       "0                                              Fast Cars   \n",
       "1                                      Watching The Rain   \n",
       "2                                       Run to the Money   \n",
       "3      Determined (Vows of Vengeance) - Live at Summe...   \n",
       "4                                         Dandelion Days   \n",
       "...                                                  ...   \n",
       "36754                                               Rain   \n",
       "36755                                  Cardiff Afterlife   \n",
       "36756                        Darkness Breeds Immortality   \n",
       "36757                                  Super Bad Sisters   \n",
       "36758                                    Topsy's Revenge   \n",
       "\n",
       "                       artist  \\\n",
       "0                 Craig David   \n",
       "1               Scapegoat Wax   \n",
       "2              Bankroll Fresh   \n",
       "3                   Kataklysm   \n",
       "4               Adam McHeffey   \n",
       "...                       ...   \n",
       "36754             Leela James   \n",
       "36755  Manic Street Preachers   \n",
       "36756                  Marduk   \n",
       "36757           Sister Sledge   \n",
       "36758          Grand Archives   \n",
       "\n",
       "                                                  lyrics  listens  hotness  \\\n",
       "0         Fast cars Fast women Speed bikes with the n...   751624       28   \n",
       "1         Hello hello its me again  You know since yo...    10681        6   \n",
       "2            Fuckin up money I already done it Guess ...   160202       17   \n",
       "3        Your fate is made of words that you spill Li...    19218        4   \n",
       "4        Well look back on our old ways And call them...     1038        0   \n",
       "...                                                  ...      ...      ...   \n",
       "36754     I cant take this rain  no When Im bringing ...   256830       19   \n",
       "36755     If the love between us has faded away Left ...    94223       15   \n",
       "36756    An angel in ecstasy crosses the sky The sky ...    69664       17   \n",
       "36757     Super bad super bad ah ah ah arent we bad S...    78248       14   \n",
       "36758    The rumours around I wont last long now My c...    57110        4   \n",
       "\n",
       "                              genres  genius ID              spotify ID  \n",
       "0      ['R&B Genius', 'Rock Genius']        NaN                     NaN  \n",
       "1                     ['Pop Genius']        NaN                     NaN  \n",
       "2                     ['Rap Genius']        NaN                     NaN  \n",
       "3                     ['Pop Genius']        NaN                     NaN  \n",
       "4                     ['Pop Genius']        NaN                     NaN  \n",
       "...                              ...        ...                     ...  \n",
       "36754         ['Soul', 'R&B Genius']   957879.0  58qUsOq28QtDpPdyHAARNQ  \n",
       "36755                ['Rock Genius']   463900.0  1SylCJI2pHEXC1GO3cd9iR  \n",
       "36756                 ['Pop Genius']  1402388.0  5RFS3Z2vLi0jOzSDAIHQnd  \n",
       "36757                 ['R&B Genius']  3988510.0  3n2aVCZTCCJ4HDIrsiTbZ4  \n",
       "36758                 ['Pop Genius']  1098025.0  70ua2E92cg7euTplfFAvYm  \n",
       "\n",
       "[36759 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cleaned_data.csv\")\n",
    "length = len(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now form a list 'index_lookup' in order to get a unique word count for our songs in the corpus. This involves removing capital letters via the \".lower()\" attribute in order to avoid a unique count mistake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lookup = {}\n",
    "x = 0\n",
    "for text in data[\"lyrics\"]:\n",
    "    for word in text.lower().split(\" \"):\n",
    "        if not word in index_lookup.keys() and not word == '':\n",
    "            index_lookup.update({word: x})\n",
    "            x = x + 1\n",
    "word_lookup = {v : k  for k , v in index_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(index_lookup)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from above we have a total of 118466 unique words in our data. Lets take the first song from our data and index its unique words as a unique numeric value, by doing this we are able to look for errors in the value assigning process. We will do this by looping over the first song and splitting the values by each space. In the end we should have a list of numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Sitting there while I observe I like your lines I love your curves Checking out your bodywork How can I get with her Youre the one that I want Do anything to turn you on Somebody please just pass the keys so you can take a ride with me   Im on a mission First thing disarming your system Next thing slip the key in the ignition Just listen To the way that you purr at me you know you prefer the speed When your back starts dipping Wheel spinning when the gears start shifting Im sticking til the turbo kicks in You know that Im missing Got me moving so fast you got me missing the flash a 50   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Feel the ride feel the rush The moment I tease your clutch Reacting to my every touch Were shifting down or tearing up I dont care where we go To burn you outs the end result Youre dealing with a professional Ill spin your round just to let you know   Im on a mission First thing disarming your system Next thing slip the key in the ignition Just listen To the way that you purr at me you know your as perverse as me When your back starts dipping And Im unaware of the limits Im hitting Blurred vision in a critical condition Could blow the transmission Got me moving so fast you got me missing the flash a 50   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Body like a race car You got the curves like Jaguar Acceleration like a Yamaha I like the way you perform when Im turning you on Feel it when I revving you Smoother than a Daimler You handle like a Lancia I like the way you perform when Im turning you on   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 17,\n",
       " 23,\n",
       " 17,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 17,\n",
       " 27,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 25,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 17,\n",
       " 34,\n",
       " 5,\n",
       " 35,\n",
       " 36,\n",
       " 6,\n",
       " 37,\n",
       " 16,\n",
       " 17,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 6,\n",
       " 49,\n",
       " 50,\n",
       " 43,\n",
       " 33,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 5,\n",
       " 54,\n",
       " 55,\n",
       " 44,\n",
       " 52,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 25,\n",
       " 60,\n",
       " 61,\n",
       " 58,\n",
       " 62,\n",
       " 6,\n",
       " 63,\n",
       " 8,\n",
       " 6,\n",
       " 64,\n",
       " 47,\n",
       " 65,\n",
       " 41,\n",
       " 6,\n",
       " 66,\n",
       " 16,\n",
       " 43,\n",
       " 67,\n",
       " 68,\n",
       " 54,\n",
       " 43,\n",
       " 69,\n",
       " 43,\n",
       " 70,\n",
       " 6,\n",
       " 3,\n",
       " 11,\n",
       " 25,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 11,\n",
       " 6,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 55,\n",
       " 79,\n",
       " 80,\n",
       " 6,\n",
       " 81,\n",
       " 82,\n",
       " 8,\n",
       " 43,\n",
       " 69,\n",
       " 16,\n",
       " 55,\n",
       " 83,\n",
       " 84,\n",
       " 54,\n",
       " 85,\n",
       " 50,\n",
       " 0,\n",
       " 43,\n",
       " 84,\n",
       " 54,\n",
       " 83,\n",
       " 6,\n",
       " 86,\n",
       " 52,\n",
       " 87,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 88,\n",
       " 6,\n",
       " 53,\n",
       " 88,\n",
       " 6,\n",
       " 89,\n",
       " 6,\n",
       " 90,\n",
       " 17,\n",
       " 91,\n",
       " 25,\n",
       " 92,\n",
       " 93,\n",
       " 41,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 78,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 17,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 41,\n",
       " 107,\n",
       " 43,\n",
       " 108,\n",
       " 6,\n",
       " 109,\n",
       " 110,\n",
       " 36,\n",
       " 111,\n",
       " 5,\n",
       " 52,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 25,\n",
       " 115,\n",
       " 47,\n",
       " 41,\n",
       " 116,\n",
       " 43,\n",
       " 69,\n",
       " 55,\n",
       " 44,\n",
       " 52,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 25,\n",
       " 60,\n",
       " 61,\n",
       " 58,\n",
       " 62,\n",
       " 6,\n",
       " 63,\n",
       " 8,\n",
       " 6,\n",
       " 64,\n",
       " 47,\n",
       " 65,\n",
       " 41,\n",
       " 6,\n",
       " 66,\n",
       " 16,\n",
       " 43,\n",
       " 67,\n",
       " 68,\n",
       " 54,\n",
       " 43,\n",
       " 69,\n",
       " 25,\n",
       " 117,\n",
       " 118,\n",
       " 117,\n",
       " 54,\n",
       " 11,\n",
       " 25,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 119,\n",
       " 55,\n",
       " 120,\n",
       " 121,\n",
       " 6,\n",
       " 122,\n",
       " 55,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 8,\n",
       " 52,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 6,\n",
       " 130,\n",
       " 84,\n",
       " 54,\n",
       " 85,\n",
       " 50,\n",
       " 0,\n",
       " 43,\n",
       " 84,\n",
       " 54,\n",
       " 83,\n",
       " 6,\n",
       " 86,\n",
       " 52,\n",
       " 87,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 131,\n",
       " 24,\n",
       " 52,\n",
       " 132,\n",
       " 133,\n",
       " 43,\n",
       " 84,\n",
       " 6,\n",
       " 28,\n",
       " 24,\n",
       " 134,\n",
       " 135,\n",
       " 24,\n",
       " 52,\n",
       " 136,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 66,\n",
       " 43,\n",
       " 137,\n",
       " 11,\n",
       " 55,\n",
       " 138,\n",
       " 43,\n",
       " 44,\n",
       " 88,\n",
       " 139,\n",
       " 11,\n",
       " 17,\n",
       " 140,\n",
       " 43,\n",
       " 141,\n",
       " 142,\n",
       " 52,\n",
       " 143,\n",
       " 43,\n",
       " 144,\n",
       " 24,\n",
       " 52,\n",
       " 145,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 66,\n",
       " 43,\n",
       " 137,\n",
       " 11,\n",
       " 55,\n",
       " 138,\n",
       " 43,\n",
       " 44,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for text in [data[\"lyrics\"][0]]:\n",
    "    songlist = []\n",
    "    print(text)\n",
    "    for word in text.lower().split(\" \"):\n",
    "        try:\n",
    "            songlist.append(index_lookup[word])\n",
    "        except:\n",
    "            pass\n",
    "songlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks great. Now we will do this over all the song lyrics in our corpus and we will have a completed list of unique words mapped to a unique numeric value for every unique value in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in data[\"lyrics\"]:\n",
    "    songlist = []\n",
    "    for word in text.lower().split(\" \"):\n",
    "        try:\n",
    "            songlist.append(index_lookup[word])\n",
    "        except:\n",
    "            pass\n",
    "    corpus.append(songlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation of Context Word Pairs\n",
    "\n",
    "Now that the corpus is complete it is time to form the pairs of target words and context words. This involves some arbitrary choice for the size of our context window, we have decided on three because of its accuracy and lower probability of overfitting our data. The first steps are to form a funciton that creates the pairs we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for lyrics in corpus:\n",
    "        lyrics_length = len(lyrics)\n",
    "        for index, word in enumerate(lyrics):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([lyrics[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < lyrics_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should yeild our pairs and the best way to determine if it does is to test it out on our first song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['women', 'speed', 'bikes', 'the', 'nitro', 'in'] -> Target (Y): with\n",
      "Context (X): ['speed', 'bikes', 'with', 'nitro', 'in', 'them'] -> Target (Y): the\n",
      "Context (X): ['bikes', 'with', 'the', 'in', 'them', 'dangerous'] -> Target (Y): nitro\n",
      "Context (X): ['with', 'the', 'nitro', 'them', 'dangerous', 'when'] -> Target (Y): in\n",
      "Context (X): ['the', 'nitro', 'in', 'dangerous', 'when', 'driven'] -> Target (Y): them\n",
      "Context (X): ['nitro', 'in', 'them', 'when', 'driven', 'those'] -> Target (Y): dangerous\n",
      "Context (X): ['in', 'them', 'dangerous', 'driven', 'those', 'are'] -> Target (Y): when\n",
      "Context (X): ['them', 'dangerous', 'when', 'those', 'are', 'the'] -> Target (Y): driven\n",
      "Context (X): ['dangerous', 'when', 'driven', 'are', 'the', 'type'] -> Target (Y): those\n",
      "Context (X): ['when', 'driven', 'those', 'the', 'type', 'that'] -> Target (Y): are\n",
      "Context (X): ['driven', 'those', 'are', 'type', 'that', 'i'] -> Target (Y): the\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [word_lookup[w] for w in x[0]], '-> Target (Y):', word_lookup[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that the funciton successfully pairs the context word with our target word. Now its time to build our CBOW modeling architecture. This involves using Keras and tensorflow in order to build. This model will take inputs of context words that are passed to an embedding layer and given random weights. The embeddings are then sent to a lambda layer where we average our the word embeddings. These new values are then passed to a dense softmax layer that attempts to predict our target word. The results given by our dense softmax layer are then compared to our actual target word and compute a loss by leveraging the the categorical_crossentropy loss and perform backpropagation with each epoch to update the embedding layer in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 100)            11846600  \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 118466)            11965066  \n",
      "=================================================================\n",
      "Total params: 23,811,666\n",
      "Trainable params: 23,811,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "embed_size = 100\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the summary we have 23,811,666 parameters! This model is now ready to be trained on our corpus. The following code shows how it will be computed however due to the size of these embeddings we will use a third party means of computation (as stated the project proposal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan Burrows\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
