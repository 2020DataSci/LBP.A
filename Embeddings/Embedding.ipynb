{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>listens</th>\n",
       "      <th>hotness</th>\n",
       "      <th>genres</th>\n",
       "      <th>genius ID</th>\n",
       "      <th>spotify ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fast Cars</td>\n",
       "      <td>Craig David</td>\n",
       "      <td>Fast cars Fast women Speed bikes with the n...</td>\n",
       "      <td>751624</td>\n",
       "      <td>28</td>\n",
       "      <td>['R&amp;B Genius', 'Rock Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Watching The Rain</td>\n",
       "      <td>Scapegoat Wax</td>\n",
       "      <td>Hello hello its me again  You know since yo...</td>\n",
       "      <td>10681</td>\n",
       "      <td>6</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Run to the Money</td>\n",
       "      <td>Bankroll Fresh</td>\n",
       "      <td>Fuckin up money I already done it Guess ...</td>\n",
       "      <td>160202</td>\n",
       "      <td>17</td>\n",
       "      <td>['Rap Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Determined (Vows of Vengeance) - Live at Summe...</td>\n",
       "      <td>Kataklysm</td>\n",
       "      <td>Your fate is made of words that you spill Li...</td>\n",
       "      <td>19218</td>\n",
       "      <td>4</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Dandelion Days</td>\n",
       "      <td>Adam McHeffey</td>\n",
       "      <td>Well look back on our old ways And call them...</td>\n",
       "      <td>1038</td>\n",
       "      <td>0</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36754</th>\n",
       "      <td>36754</td>\n",
       "      <td>36754</td>\n",
       "      <td>Rain</td>\n",
       "      <td>Leela James</td>\n",
       "      <td>I cant take this rain  no When Im bringing ...</td>\n",
       "      <td>256830</td>\n",
       "      <td>19</td>\n",
       "      <td>['Soul', 'R&amp;B Genius']</td>\n",
       "      <td>957879.0</td>\n",
       "      <td>58qUsOq28QtDpPdyHAARNQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36755</th>\n",
       "      <td>36755</td>\n",
       "      <td>36755</td>\n",
       "      <td>Cardiff Afterlife</td>\n",
       "      <td>Manic Street Preachers</td>\n",
       "      <td>If the love between us has faded away Left ...</td>\n",
       "      <td>94223</td>\n",
       "      <td>15</td>\n",
       "      <td>['Rock Genius']</td>\n",
       "      <td>463900.0</td>\n",
       "      <td>1SylCJI2pHEXC1GO3cd9iR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36756</th>\n",
       "      <td>36756</td>\n",
       "      <td>36756</td>\n",
       "      <td>Darkness Breeds Immortality</td>\n",
       "      <td>Marduk</td>\n",
       "      <td>An angel in ecstasy crosses the sky The sky ...</td>\n",
       "      <td>69664</td>\n",
       "      <td>17</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>1402388.0</td>\n",
       "      <td>5RFS3Z2vLi0jOzSDAIHQnd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36757</th>\n",
       "      <td>36757</td>\n",
       "      <td>36757</td>\n",
       "      <td>Super Bad Sisters</td>\n",
       "      <td>Sister Sledge</td>\n",
       "      <td>Super bad super bad ah ah ah arent we bad S...</td>\n",
       "      <td>78248</td>\n",
       "      <td>14</td>\n",
       "      <td>['R&amp;B Genius']</td>\n",
       "      <td>3988510.0</td>\n",
       "      <td>3n2aVCZTCCJ4HDIrsiTbZ4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36758</th>\n",
       "      <td>36758</td>\n",
       "      <td>36758</td>\n",
       "      <td>Topsy's Revenge</td>\n",
       "      <td>Grand Archives</td>\n",
       "      <td>The rumours around I wont last long now My c...</td>\n",
       "      <td>57110</td>\n",
       "      <td>4</td>\n",
       "      <td>['Pop Genius']</td>\n",
       "      <td>1098025.0</td>\n",
       "      <td>70ua2E92cg7euTplfFAvYm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36759 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  \\\n",
       "0               0             0   \n",
       "1               1             1   \n",
       "2               2             2   \n",
       "3               3             3   \n",
       "4               4             4   \n",
       "...           ...           ...   \n",
       "36754       36754         36754   \n",
       "36755       36755         36755   \n",
       "36756       36756         36756   \n",
       "36757       36757         36757   \n",
       "36758       36758         36758   \n",
       "\n",
       "                                                   title  \\\n",
       "0                                              Fast Cars   \n",
       "1                                      Watching The Rain   \n",
       "2                                       Run to the Money   \n",
       "3      Determined (Vows of Vengeance) - Live at Summe...   \n",
       "4                                         Dandelion Days   \n",
       "...                                                  ...   \n",
       "36754                                               Rain   \n",
       "36755                                  Cardiff Afterlife   \n",
       "36756                        Darkness Breeds Immortality   \n",
       "36757                                  Super Bad Sisters   \n",
       "36758                                    Topsy's Revenge   \n",
       "\n",
       "                       artist  \\\n",
       "0                 Craig David   \n",
       "1               Scapegoat Wax   \n",
       "2              Bankroll Fresh   \n",
       "3                   Kataklysm   \n",
       "4               Adam McHeffey   \n",
       "...                       ...   \n",
       "36754             Leela James   \n",
       "36755  Manic Street Preachers   \n",
       "36756                  Marduk   \n",
       "36757           Sister Sledge   \n",
       "36758          Grand Archives   \n",
       "\n",
       "                                                  lyrics  listens  hotness  \\\n",
       "0         Fast cars Fast women Speed bikes with the n...   751624       28   \n",
       "1         Hello hello its me again  You know since yo...    10681        6   \n",
       "2            Fuckin up money I already done it Guess ...   160202       17   \n",
       "3        Your fate is made of words that you spill Li...    19218        4   \n",
       "4        Well look back on our old ways And call them...     1038        0   \n",
       "...                                                  ...      ...      ...   \n",
       "36754     I cant take this rain  no When Im bringing ...   256830       19   \n",
       "36755     If the love between us has faded away Left ...    94223       15   \n",
       "36756    An angel in ecstasy crosses the sky The sky ...    69664       17   \n",
       "36757     Super bad super bad ah ah ah arent we bad S...    78248       14   \n",
       "36758    The rumours around I wont last long now My c...    57110        4   \n",
       "\n",
       "                              genres  genius ID              spotify ID  \n",
       "0      ['R&B Genius', 'Rock Genius']        NaN                     NaN  \n",
       "1                     ['Pop Genius']        NaN                     NaN  \n",
       "2                     ['Rap Genius']        NaN                     NaN  \n",
       "3                     ['Pop Genius']        NaN                     NaN  \n",
       "4                     ['Pop Genius']        NaN                     NaN  \n",
       "...                              ...        ...                     ...  \n",
       "36754         ['Soul', 'R&B Genius']   957879.0  58qUsOq28QtDpPdyHAARNQ  \n",
       "36755                ['Rock Genius']   463900.0  1SylCJI2pHEXC1GO3cd9iR  \n",
       "36756                 ['Pop Genius']  1402388.0  5RFS3Z2vLi0jOzSDAIHQnd  \n",
       "36757                 ['R&B Genius']  3988510.0  3n2aVCZTCCJ4HDIrsiTbZ4  \n",
       "36758                 ['Pop Genius']  1098025.0  70ua2E92cg7euTplfFAvYm  \n",
       "\n",
       "[36759 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cleaned_data (1).csv\")\n",
    "length = len(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lookup = {}\n",
    "x = 0\n",
    "for text in data[\"lyrics\"]:\n",
    "    for word in text.lower().split(\" \"):\n",
    "        if not word in index_lookup.keys() and not word == '':\n",
    "            index_lookup.update({word: x})\n",
    "            x = x + 1\n",
    "word_lookup = {v : k  for k , v in index_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118466"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(index_lookup)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Sitting there while I observe I like your lines I love your curves Checking out your bodywork How can I get with her Youre the one that I want Do anything to turn you on Somebody please just pass the keys so you can take a ride with me   Im on a mission First thing disarming your system Next thing slip the key in the ignition Just listen To the way that you purr at me you know you prefer the speed When your back starts dipping Wheel spinning when the gears start shifting Im sticking til the turbo kicks in You know that Im missing Got me moving so fast you got me missing the flash a 50   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Feel the ride feel the rush The moment I tease your clutch Reacting to my every touch Were shifting down or tearing up I dont care where we go To burn you outs the end result Youre dealing with a professional Ill spin your round just to let you know   Im on a mission First thing disarming your system Next thing slip the key in the ignition Just listen To the way that you purr at me you know your as perverse as me When your back starts dipping And Im unaware of the limits Im hitting Blurred vision in a critical condition Could blow the transmission Got me moving so fast you got me missing the flash a 50   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling    Body like a race car You got the curves like Jaguar Acceleration like a Yamaha I like the way you perform when Im turning you on Feel it when I revving you Smoother than a Daimler You handle like a Lancia I like the way you perform when Im turning you on   Fast cars Fast women Speed bikes with the nitro in them Dangerous when driven Those are the type that I be feeling   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 17,\n",
       " 23,\n",
       " 17,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 17,\n",
       " 27,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 25,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 17,\n",
       " 34,\n",
       " 5,\n",
       " 35,\n",
       " 36,\n",
       " 6,\n",
       " 37,\n",
       " 16,\n",
       " 17,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 6,\n",
       " 49,\n",
       " 50,\n",
       " 43,\n",
       " 33,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 5,\n",
       " 54,\n",
       " 55,\n",
       " 44,\n",
       " 52,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 25,\n",
       " 60,\n",
       " 61,\n",
       " 58,\n",
       " 62,\n",
       " 6,\n",
       " 63,\n",
       " 8,\n",
       " 6,\n",
       " 64,\n",
       " 47,\n",
       " 65,\n",
       " 41,\n",
       " 6,\n",
       " 66,\n",
       " 16,\n",
       " 43,\n",
       " 67,\n",
       " 68,\n",
       " 54,\n",
       " 43,\n",
       " 69,\n",
       " 43,\n",
       " 70,\n",
       " 6,\n",
       " 3,\n",
       " 11,\n",
       " 25,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 11,\n",
       " 6,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 55,\n",
       " 79,\n",
       " 80,\n",
       " 6,\n",
       " 81,\n",
       " 82,\n",
       " 8,\n",
       " 43,\n",
       " 69,\n",
       " 16,\n",
       " 55,\n",
       " 83,\n",
       " 84,\n",
       " 54,\n",
       " 85,\n",
       " 50,\n",
       " 0,\n",
       " 43,\n",
       " 84,\n",
       " 54,\n",
       " 83,\n",
       " 6,\n",
       " 86,\n",
       " 52,\n",
       " 87,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 88,\n",
       " 6,\n",
       " 53,\n",
       " 88,\n",
       " 6,\n",
       " 89,\n",
       " 6,\n",
       " 90,\n",
       " 17,\n",
       " 91,\n",
       " 25,\n",
       " 92,\n",
       " 93,\n",
       " 41,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 78,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 17,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 41,\n",
       " 107,\n",
       " 43,\n",
       " 108,\n",
       " 6,\n",
       " 109,\n",
       " 110,\n",
       " 36,\n",
       " 111,\n",
       " 5,\n",
       " 52,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 25,\n",
       " 115,\n",
       " 47,\n",
       " 41,\n",
       " 116,\n",
       " 43,\n",
       " 69,\n",
       " 55,\n",
       " 44,\n",
       " 52,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 25,\n",
       " 60,\n",
       " 61,\n",
       " 58,\n",
       " 62,\n",
       " 6,\n",
       " 63,\n",
       " 8,\n",
       " 6,\n",
       " 64,\n",
       " 47,\n",
       " 65,\n",
       " 41,\n",
       " 6,\n",
       " 66,\n",
       " 16,\n",
       " 43,\n",
       " 67,\n",
       " 68,\n",
       " 54,\n",
       " 43,\n",
       " 69,\n",
       " 25,\n",
       " 117,\n",
       " 118,\n",
       " 117,\n",
       " 54,\n",
       " 11,\n",
       " 25,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 119,\n",
       " 55,\n",
       " 120,\n",
       " 121,\n",
       " 6,\n",
       " 122,\n",
       " 55,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 8,\n",
       " 52,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 6,\n",
       " 130,\n",
       " 84,\n",
       " 54,\n",
       " 85,\n",
       " 50,\n",
       " 0,\n",
       " 43,\n",
       " 84,\n",
       " 54,\n",
       " 83,\n",
       " 6,\n",
       " 86,\n",
       " 52,\n",
       " 87,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 131,\n",
       " 24,\n",
       " 52,\n",
       " 132,\n",
       " 133,\n",
       " 43,\n",
       " 84,\n",
       " 6,\n",
       " 28,\n",
       " 24,\n",
       " 134,\n",
       " 135,\n",
       " 24,\n",
       " 52,\n",
       " 136,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 66,\n",
       " 43,\n",
       " 137,\n",
       " 11,\n",
       " 55,\n",
       " 138,\n",
       " 43,\n",
       " 44,\n",
       " 88,\n",
       " 139,\n",
       " 11,\n",
       " 17,\n",
       " 140,\n",
       " 43,\n",
       " 141,\n",
       " 142,\n",
       " 52,\n",
       " 143,\n",
       " 43,\n",
       " 144,\n",
       " 24,\n",
       " 52,\n",
       " 145,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 66,\n",
       " 43,\n",
       " 137,\n",
       " 11,\n",
       " 55,\n",
       " 138,\n",
       " 43,\n",
       " 44,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for text in [data[\"lyrics\"][0]]:\n",
    "    songlist = []\n",
    "    print(text)\n",
    "    for word in text.lower().split(\" \"):\n",
    "        try:\n",
    "            songlist.append(index_lookup[word])\n",
    "        except:\n",
    "            pass\n",
    "songlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in data[\"lyrics\"]:\n",
    "    songlist = []\n",
    "    for word in text.lower().split(\" \"):\n",
    "        try:\n",
    "            songlist.append(index_lookup[word])\n",
    "        except:\n",
    "            pass\n",
    "    corpus.append(songlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for lyrics in corpus:\n",
    "        lyrics_length = len(lyrics)\n",
    "        for index, word in enumerate(lyrics):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([lyrics[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < lyrics_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['women', 'speed', 'bikes', 'the', 'nitro', 'in'] -> Target (Y): with\n",
      "Context (X): ['speed', 'bikes', 'with', 'nitro', 'in', 'them'] -> Target (Y): the\n",
      "Context (X): ['bikes', 'with', 'the', 'in', 'them', 'dangerous'] -> Target (Y): nitro\n",
      "Context (X): ['with', 'the', 'nitro', 'them', 'dangerous', 'when'] -> Target (Y): in\n",
      "Context (X): ['the', 'nitro', 'in', 'dangerous', 'when', 'driven'] -> Target (Y): them\n",
      "Context (X): ['nitro', 'in', 'them', 'when', 'driven', 'those'] -> Target (Y): dangerous\n",
      "Context (X): ['in', 'them', 'dangerous', 'driven', 'those', 'are'] -> Target (Y): when\n",
      "Context (X): ['them', 'dangerous', 'when', 'those', 'are', 'the'] -> Target (Y): driven\n",
      "Context (X): ['dangerous', 'when', 'driven', 'are', 'the', 'type'] -> Target (Y): those\n",
      "Context (X): ['when', 'driven', 'those', 'the', 'type', 'that'] -> Target (Y): are\n",
      "Context (X): ['driven', 'those', 'are', 'type', 'that', 'i'] -> Target (Y): the\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [word_lookup[w] for w in x[0]], '-> Target (Y):', word_lookup[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 100)            11846600  \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 118466)            11965066  \n",
      "=================================================================\n",
      "Total params: 23,811,666\n",
      "Trainable params: 23,811,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "embed_size = 100\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TannerSims\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5244ca4ee64b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate_context_word_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed {} (context, word) pairs'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3748\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3749\u001b[0m         expand_composites=True)\n\u001b[0;32m   3750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3748\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3749\u001b[0m         expand_composites=True)\n\u001b[0;32m   3750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=corpus, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
